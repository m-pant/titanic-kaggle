#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö ML –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ Titanic
–ê–≤—Ç–æ—Ä: Claude AI Assistant
–û–ø–∏—Å–∞–Ω–∏–µ: –ü–æ–ª–Ω—ã–π pipeline –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏, –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
"""

import numpy as np
import pandas as pd
import warnings
import time
from datetime import datetime

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
import seaborn as sns

# Sklearn
from sklearn.model_selection import (
    train_test_split, cross_val_score, StratifiedKFold,
    GridSearchCV, RandomizedSearchCV
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, roc_auc_score, confusion_matrix,
    classification_report, roc_curve, auc
)

# –ú–æ–¥–µ–ª–∏
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    ExtraTreesClassifier, AdaBoostClassifier,
    VotingClassifier, StackingClassifier
)
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("‚ö†Ô∏è XGBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install xgboost")

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("‚ö†Ô∏è LightGBM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install lightgbm")

try:
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("‚ö†Ô∏è CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install catboost")

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
plt.style.use('seaborn-v0_8-darkgrid')

# ======================== –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ========================

def load_titanic_data():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Titanic —Å Kaggle –∏–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    """
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        train_df = pd.read_csv('data/train.csv')
        test_df = pd.read_csv('data/test.csv')
        print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    except FileNotFoundError:
        print("üì• –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ —Å GitHub (–ø—É–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)...")
        train_url = "https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv"
        try:
            train_df = pd.read_csv(train_url)
            # –î–ª—è GitHub –≤–µ—Ä—Å–∏–∏ –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ train/test –≤—Ä—É—á–Ω—É—é
            test_df = None
            print("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å GitHub")
        except:
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏. –°–∫–∞—á–∞–π—Ç–µ train.csv —Å Kaggle: https://www.kaggle.com/c/titanic")
            return None, None
    
    return train_df, test_df


def preprocess_titanic_data(train_df, test_df=None):
    """
    –ü–æ–ª–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Titanic
    """
    print("\nüîß –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•")
    print("="*60)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –µ–¥–∏–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å test)
    if test_df is not None:
        df_all = pd.concat([train_df, test_df], sort=False)
        len_train = len(train_df)
    else:
        df_all = train_df.copy()
        len_train = len(train_df)
    
    # ========== 1. FEATURE ENGINEERING ==========
    
    # –†–∞–∑–º–µ—Ä —Å–µ–º—å–∏
    df_all['FamilySize'] = df_all['SibSp'] + df_all['Parch'] + 1
    
    # –ü—É—Ç–µ—à–µ—Å—Ç–≤—É–µ—Ç –æ–¥–∏–Ω?
    df_all['IsAlone'] = (df_all['FamilySize'] == 1).astype(int)
    
    # –¢–∏—Ç—É–ª –∏–∑ –∏–º–µ–Ω–∏
    df_all['Title'] = df_all['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–¥–∫–∏—Ö —Ç–∏—Ç—É–ª–æ–≤
    title_mapping = {
        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
        'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
        'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',
        'Don': 'Rare', 'Dona': 'Rare', 'Lady': 'Rare', 'Countess': 'Rare',
        'Capt': 'Rare', 'Sir': 'Rare', 'Jonkheer': 'Rare'
    }
    df_all['Title'] = df_all['Title'].map(title_mapping).fillna('Rare')
    
    # Deck –∏–∑ –∫–∞—é—Ç—ã
    df_all['Deck'] = df_all['Cabin'].str[0]
    df_all['Deck'] = df_all['Deck'].fillna('Unknown')
    
    # –ì—Ä—É–ø–ø—ã –≤–æ–∑—Ä–∞—Å—Ç–æ–≤
    df_all['AgeGroup'] = pd.cut(df_all['Age'], 
                                bins=[0, 12, 18, 35, 60, 100],
                                labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])
    
    # –ì—Ä—É–ø–ø—ã —Ç–∞—Ä–∏—Ñ–æ–≤
    df_all['FareGroup'] = pd.qcut(df_all['Fare'].fillna(df_all['Fare'].median()), 
                                  q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])
    
    # ========== 2. –ó–ê–ü–û–õ–ù–ï–ù–ò–ï –ü–†–û–ü–£–°–ö–û–í ==========
    
    # –í–æ–∑—Ä–∞—Å—Ç - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π –ø–æ –≥—Ä—É–ø–ø–∞–º
    age_medians = df_all.groupby(['Title', 'Pclass'])['Age'].transform('median')
    df_all['Age'] = df_all['Age'].fillna(age_medians)
    df_all['Age'] = df_all['Age'].fillna(df_all['Age'].median())
    
    # Embarked - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–æ–¥–æ–π
    df_all['Embarked'] = df_all['Embarked'].fillna(df_all['Embarked'].mode()[0])
    
    # Fare - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π –ø–æ –∫–ª–∞—Å—Å—É
    fare_medians = df_all.groupby('Pclass')['Fare'].transform('median')
    df_all['Fare'] = df_all['Fare'].fillna(fare_medians)
    
    # ========== 3. –ö–û–î–ò–†–û–í–ê–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô ==========
    
    # Label Encoding –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö
    le = LabelEncoder()
    df_all['Sex'] = le.fit_transform(df_all['Sex'])
    
    # One-Hot Encoding –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    categorical_features = ['Embarked', 'Title', 'Deck', 'AgeGroup', 'FareGroup']
    df_all = pd.get_dummies(df_all, columns=categorical_features, drop_first=False)
    
    # ========== 4. –í–´–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í ==========
    
    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin']
    df_all = df_all.drop([col for col in drop_columns if col in df_all.columns], axis=1)
    
    # ========== 5. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –û–ë–†–ê–¢–ù–û ==========
    
    if test_df is not None:
        train = df_all[:len_train].copy()
        test = df_all[len_train:].copy()
        
        # –£–±–∏—Ä–∞–µ–º Survived –∏–∑ test –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'Survived' in test.columns:
            test = test.drop('Survived', axis=1)
        
        return train, test
    else:
        return df_all, None


def prepare_features(df):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
    """
    if 'Survived' in df.columns:
        X = df.drop('Survived', axis=1)
        y = df['Survived']
        return X, y
    else:
        return df, None


# ======================== –ú–û–î–ï–õ–ò –ò –ò–• –ù–ê–°–¢–†–û–ô–ö–ò ========================

class ModelComparison:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = self._initialize_models()
        self.results = []
        
    def _initialize_models(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è Titanic
        """
        models = {}
        
        # 1. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        models['Logistic Regression'] = {
            'model': LogisticRegression(
                C=0.1, penalty='l2', max_iter=1000,
                random_state=self.random_state
            ),
            'needs_scaling': True
        }
        
        # 2. Random Forest
        models['Random Forest'] = {
            'model': RandomForestClassifier(
                n_estimators=100, max_depth=5,
                min_samples_split=10, min_samples_leaf=5,
                max_features='sqrt', random_state=self.random_state
            ),
            'needs_scaling': False
        }
        
        # 3. Gradient Boosting
        models['Gradient Boosting'] = {
            'model': GradientBoostingClassifier(
                n_estimators=100, learning_rate=0.05, max_depth=3,
                min_samples_split=10, min_samples_leaf=5,
                subsample=0.8, random_state=self.random_state
            ),
            'needs_scaling': False
        }
        
        # 4. XGBoost
        if XGBOOST_AVAILABLE:
            models['XGBoost'] = {
                'model': XGBClassifier(
                    n_estimators=100, learning_rate=0.05, max_depth=3,
                    min_child_weight=5, subsample=0.8, colsample_bytree=0.8,
                    gamma=1, reg_alpha=1, reg_lambda=2,
                    random_state=self.random_state,
                    use_label_encoder=False, eval_metric='logloss'
                ),
                'needs_scaling': False
            }
        
        # 5. LightGBM
        if LIGHTGBM_AVAILABLE:
            models['LightGBM'] = {
                'model': LGBMClassifier(
                    n_estimators=100, num_leaves=15, learning_rate=0.05,
                    min_child_samples=20, subsample=0.8, colsample_bytree=0.8,
                    reg_alpha=1, reg_lambda=2,
                    random_state=self.random_state, verbose=-1
                ),
                'needs_scaling': False
            }
        
        # 6. CatBoost
        if CATBOOST_AVAILABLE:
            models['CatBoost'] = {
                'model': CatBoostClassifier(
                    iterations=100, learning_rate=0.05, depth=4,
                    l2_leaf_reg=5, border_count=32,
                    random_state=self.random_state, verbose=False
                ),
                'needs_scaling': False
            }
        
        # 7. SVM
        models['SVM'] = {
            'model': SVC(
                C=1.0, kernel='rbf', gamma='scale',
                probability=True, random_state=self.random_state
            ),
            'needs_scaling': True
        }
        
        # 8. KNN
        models['KNN'] = {
            'model': KNeighborsClassifier(
                n_neighbors=10, weights='distance', metric='minkowski'
            ),
            'needs_scaling': True
        }
        
        # 9. Naive Bayes
        models['Naive Bayes'] = {
            'model': GaussianNB(),
            'needs_scaling': False
        }
        
        # 10. Extra Trees
        models['Extra Trees'] = {
            'model': ExtraTreesClassifier(
                n_estimators=100, max_depth=5,
                min_samples_split=10, min_samples_leaf=5,
                random_state=self.random_state
            ),
            'needs_scaling': False
        }
        
        # 11. AdaBoost
        models['AdaBoost'] = {
            'model': AdaBoostClassifier(
                n_estimators=100, learning_rate=0.5,
                random_state=self.random_state
            ),
            'needs_scaling': False
        }
        
        # 12. Neural Network
        models['Neural Network'] = {
            'model': MLPClassifier(
                hidden_layer_sizes=(50, 30), activation='relu',
                solver='adam', alpha=0.01, learning_rate='adaptive',
                max_iter=1000, early_stopping=True,
                validation_fraction=0.2, random_state=self.random_state
            ),
            'needs_scaling': True
        }
        
        return models
    
    def train_and_evaluate(self, X_train, y_train, X_val, y_val, X_test=None, cv_folds=5):
        """
        –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        print("\nüî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
        print("="*115)
        print(f"{'–ú–æ–¥–µ–ª—å':<25} {'CV AUC':>10} {'Val ACC':>10} {'Val AUC':>10} {'Kaggle ACC':>12} {'–í—Ä–µ–º—è (—Å)':>12}")
        print("-" * 115)

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        if X_test is not None:
            X_test_scaled = scaler.transform(X_test)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Kaggle
        real_data = None
        if X_test is not None:
            try:
                real_data = pd.read_csv('cheats/_correct_submission.csv')
            except FileNotFoundError:
                print("‚ö†Ô∏è –§–∞–π–ª 'cheats/_correct_submission.csv' –Ω–µ –Ω–∞–π–¥–µ–Ω. –†–µ–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å Kaggle –Ω–µ –±—É–¥–µ—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞.")

        # Stratified K-Fold
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)

        for name, model_info in self.models.items():
            start_time = time.time()

            # –í—ã–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if model_info['needs_scaling']:
                X_tr, X_vl = X_train_scaled, X_val_scaled
            else:
                X_tr, X_vl = X_train, X_val

            try:
                # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                cv_scores = cross_val_score(
                    model_info['model'], X_tr, y_train,
                    cv=skf, scoring='roc_auc', n_jobs=-1
                )

                # –û–±—É—á–µ–Ω–∏–µ
                model_info['model'].fit(X_tr, y_train)

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                y_pred = model_info['model'].predict(X_vl)
                y_proba = model_info['model'].predict_proba(X_vl)[:, 1]

                # –ú–µ—Ç—Ä–∏–∫–∏
                val_acc = accuracy_score(y_val, y_pred)
                val_auc = roc_auc_score(y_val, y_proba)

                # Real KAGGLE accuracy (–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
                real_accuracy = np.nan
                if X_test is not None and real_data is not None:
                    if model_info['needs_scaling']:
                        X_ts = X_test_scaled
                    else:
                        X_ts = X_test
                    y_test_pred = model_info['model'].predict(X_ts)
                    if len(real_data) == len(y_test_pred):
                        real_accuracy = (real_data["Survived"] == y_test_pred).mean()

                elapsed_time = time.time() - start_time

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                self.results.append({
                    'Model': name,
                    'CV_AUC_Mean': cv_scores.mean(),
                    'CV_AUC_Std': cv_scores.std(),
                    'Val_Accuracy': val_acc,
                    'Val_AUC': val_auc,
                    'Kaggle_Accuracy': real_accuracy,
                    'Training_Time': elapsed_time,
                    'model_object': model_info['model']
                })

                print(f"{name:<25} {cv_scores.mean():>10.4f} {val_acc:>10.4f} {val_auc:>10.4f} {real_accuracy:>12.4f} {elapsed_time:>12.2f}")

            except Exception as e:
                print(f"{name:<25} {'–û–®–ò–ë–ö–ê':<10} - {str(e)[:50]}")

        print("-" * 115)

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        self.results_df = pd.DataFrame(self.results)
        self.results_df = self.results_df.sort_values('Val_AUC', ascending=False)

        return self.results_df
    
    def plot_results(self):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        if not self.results:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        df = self.results_df
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ AUC
        ax1 = axes[0, 0]
        x_pos = np.arange(len(df))
        ax1.barh(x_pos, df['Val_AUC'], color='skyblue', label='Validation AUC')
        ax1.barh(x_pos, df['CV_AUC_Mean'], alpha=0.6, color='orange', label='CV AUC Mean')
        ax1.set_yticks(x_pos)
        ax1.set_yticklabels(df['Model'])
        ax1.set_xlabel('AUC Score')
        ax1.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ AUC –º–æ–¥–µ–ª–µ–π')
        ax1.legend()
        ax1.grid(alpha=0.3)
        
        # 2. Accuracy vs AUC
        ax2 = axes[0, 1]
        ax2.scatter(df['Val_Accuracy'], df['Val_AUC'], s=100, alpha=0.6)
        for i, txt in enumerate(df['Model']):
            ax2.annotate(txt, (df['Val_Accuracy'].iloc[i], df['Val_AUC'].iloc[i]),
                        fontsize=8, ha='right')
        ax2.set_xlabel('Validation Accuracy')
        ax2.set_ylabel('Validation AUC')
        ax2.set_title('Accuracy vs AUC')
        ax2.grid(alpha=0.3)
        
        # 3. –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        ax3 = axes[1, 0]
        ax3.barh(x_pos, df['Training_Time'], color='green', alpha=0.6)
        ax3.set_yticks(x_pos)
        ax3.set_yticklabels(df['Model'])
        ax3.set_xlabel('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫—É–Ω–¥—ã)')
        ax3.set_title('–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π')
        ax3.grid(alpha=0.3)
        
        # 4. –¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π
        ax4 = axes[1, 1]
        top5 = df.head(5)
        colors = ['gold', 'silver', '#CD7F32', 'lightblue', 'lightgreen']
        bars = ax4.bar(range(len(top5)), top5['Val_AUC'], color=colors)
        ax4.set_xticks(range(len(top5)))
        ax4.set_xticklabels(top5['Model'], rotation=45, ha='right')
        ax4.set_ylabel('Validation AUC')
        ax4.set_title('–¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π –ø–æ AUC')
        ax4.set_ylim([top5['Val_AUC'].min() - 0.01, top5['Val_AUC'].max() + 0.01])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars, top5['Val_AUC']):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{value:.4f}', ha='center', va='bottom')
        
        plt.suptitle('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –Ω–∞ Titanic', fontsize=16, y=1.02)
        plt.tight_layout()
        plt.show()
        
        return fig
    
    def create_ensemble(self, top_n=3):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        """
        if len(self.results_df) < top_n:
            print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è. –î–æ—Å—Ç—É–ø–Ω–æ: {len(self.results_df)}")
            return None
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-N –º–æ–¥–µ–ª–µ–π
        top_models = self.results_df.head(top_n)
        
        # –°–æ–∑–¥–∞–µ–º Voting Classifier
        estimators = [(row['Model'], row['model_object']) 
                     for _, row in top_models.iterrows()]
        
        voting_clf = VotingClassifier(estimators=estimators, voting='soft')
        
        print(f"\nüéØ –°–æ–∑–¥–∞–Ω –∞–Ω—Å–∞–º–±–ª—å –∏–∑ —Ç–æ–ø-{top_n} –º–æ–¥–µ–ª–µ–π:")
        for model_name in top_models['Model']:
            print(f"  - {model_name}")
        
        return voting_clf
    
    def get_best_model(self):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        """
        if self.results_df.empty:
            print("‚ùå –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            return None
        
        best = self.results_df.iloc[0]
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best['Model']}")
        print(f"   Val AUC: {best['Val_AUC']:.4f}")
        print(f"   Val Accuracy: {best['Val_Accuracy']:.4f}")
        print(f"   CV AUC: {best['CV_AUC_Mean']:.4f} (¬±{best['CV_AUC_Std']:.4f})")
        
        return best['model_object']


# ======================== –ü–†–û–î–í–ò–ù–£–¢–´–ï –ú–ï–¢–û–î–´ ========================

class AdvancedEnsembles:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã
    """
    
    def __init__(self, base_models, random_state=42):
        self.base_models = base_models
        self.random_state = random_state
    
    def create_stacking_ensemble(self, X_train, y_train):
        """
        Stacking —Å –º–µ—Ç–∞-–º–æ–¥–µ–ª—å—é
        """
        # –ë–µ—Ä–µ–º —Ç–æ–ø-4 –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        estimators = list(self.base_models.items())[:4]
        
        stacking_clf = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(C=0.1),
            cv=5,
            n_jobs=-1
        )
        
        print("\nüìö –û–±—É—á–∞—é Stacking –∞–Ω—Å–∞–º–±–ª—å...")
        stacking_clf.fit(X_train, y_train)
        
        return stacking_clf
    
    def create_blending_ensemble(self, models, weights=None):
        """
        Blending - –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ
        """
        class BlendingClassifier:
            def __init__(self, models, weights=None):
                self.models = models
                self.weights = weights if weights else [1/len(models)] * len(models)
            
            def fit(self, X, y):
                for name, model in self.models:
                    model.fit(X, y)
                return self
            
            def predict_proba(self, X):
                predictions = np.zeros((X.shape[0], 2))
                for (name, model), weight in zip(self.models, self.weights):
                    predictions += model.predict_proba(X) * weight
                return predictions
            
            def predict(self, X):
                return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)
        
        return BlendingClassifier(models, weights)


# ======================== HYPERPARAMETER TUNING ========================

def tune_best_model(model_name, X_train, y_train, X_val, y_val):
    """
    –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    """
    print(f"\nüéØ –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ {model_name}...")
    
    param_grids = {
        'LightGBM': {
            'n_estimators': [100, 150, 200],
            'num_leaves': [10, 15, 20, 25],
            'learning_rate': [0.03, 0.05, 0.08],
            'min_child_samples': [15, 20, 25],
            'subsample': [0.7, 0.8, 0.9],
            'colsample_bytree': [0.7, 0.8, 0.9]
        },
        'XGBoost': {
            'n_estimators': [100, 150, 200],
            'max_depth': [3, 4, 5],
            'learning_rate': [0.03, 0.05, 0.08],
            'min_child_weight': [3, 5, 7],
            'subsample': [0.7, 0.8, 0.9],
            'colsample_bytree': [0.7, 0.8, 0.9]
        },
        'CatBoost': {
            'iterations': [100, 150, 200],
            'depth': [3, 4, 5],
            'learning_rate': [0.03, 0.05, 0.08],
            'l2_leaf_reg': [1, 3, 5, 7]
        }
    }
    
    if model_name not in param_grids:
        print(f"‚ö†Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {model_name} –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º RandomizedSearch –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    if model_name == 'LightGBM' and LIGHTGBM_AVAILABLE:
        base_model = LGBMClassifier(random_state=42, verbose=-1)
    elif model_name == 'XGBoost' and XGBOOST_AVAILABLE:
        base_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
    elif model_name == 'CatBoost' and CATBOOST_AVAILABLE:
        base_model = CatBoostClassifier(random_state=42, verbose=False)
    else:
        return None
    
    random_search = RandomizedSearchCV(
        base_model,
        param_distributions=param_grids[model_name],
        n_iter=20,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    
    random_search.fit(X_train, y_train)
    
    print(f"\n‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {random_search.best_params_}")
    print(f"   –õ—É—á—à–∏–π CV Score: {random_search.best_score_:.4f}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    y_val_pred = random_search.predict(X_val)
    y_val_proba = random_search.predict_proba(X_val)[:, 1]
    
    val_acc = accuracy_score(y_val, y_val_pred)
    val_auc = roc_auc_score(y_val, y_val_proba)
    
    print(f"   Val Accuracy: {val_acc:.4f}")
    print(f"   Val AUC: {val_auc:.4f}")
    
    return random_search.best_estimator_


# ======================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ========================

def main():
    """
    –û—Å–Ω–æ–≤–Ω–æ–π pipeline
    """
    print("="*100)
    print(" "*30 + "üö¢ TITANIC ML COMPARISON üö¢")
    print("="*100)
    print(f"–ó–∞–ø—É—â–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìÅ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("-"*60)
    train_df, test_df = load_titanic_data()
    
    if train_df is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return
    
    print(f"   –†–∞–∑–º–µ—Ä train: {train_df.shape}")
    if test_df is not None:
        print(f"   –†–∞–∑–º–µ—Ä test: {test_df.shape}")
    
    # 2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    train_processed, test_processed = preprocess_titanic_data(train_df, test_df)
    
    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y = prepare_features(train_processed)
    X_test = None
    if test_processed is not None:
        X_test, _ = prepare_features(test_processed)

    print(f"\n   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {X.shape[1]}")
    print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: ")
    print(f"      - –í—ã–∂–∏–≤—à–∏–µ: {y.sum()} ({y.mean():.1%})")
    print(f"      - –ü–æ–≥–∏–±—à–∏–µ: {len(y) - y.sum()} ({1-y.mean():.1%})")
    
    # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, 
        stratify=y
    )
    
    print(f"\n   Train set: {X_train.shape}")
    print(f"   Validation set: {X_val.shape}")
    if X_test is not None:
        print(f"   Test set: {X_test.shape}")
    
    # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    comparison = ModelComparison(random_state=42)
    results_df = comparison.train_and_evaluate(X_train, y_train, X_val, y_val, X_test=X_test)
    
    # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("-"*60)
    comparison.plot_results()
    
    # 7. –í—ã–≤–æ–¥ —Ç–æ–ø-5 –º–æ–¥–µ–ª–µ–π
    print("\nüèÜ –¢–û–ü-5 –ú–û–î–ï–õ–ï–ô –ü–û VALIDATION AUC:")
    print("-"*60)
    top5 = results_df.head(5)
    for idx, (_, row) in enumerate(top5.iterrows(), 1):
        medal = ['ü•á', 'ü•à', 'ü•â', '4Ô∏è‚É£', '5Ô∏è‚É£'][idx-1]
        print(f"{medal} {row['Model']:<20} AUC: {row['Val_AUC']:.4f} | "
              f"Accuracy: {row['Val_Accuracy']:.4f} | "
              f"–í—Ä–µ–º—è: {row['Training_Time']:.2f}—Å")
    
    # 8. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
    best_model = comparison.get_best_model()
    best_model_name = results_df.iloc[0]['Model']
    
    # 9. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–π
    print("\nüîß –°–û–ó–î–ê–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–ï–ô")
    print("-"*60)
    
    # Voting ensemble
    voting_ensemble = comparison.create_ensemble(top_n=3)
    if voting_ensemble:
        voting_ensemble.fit(X_train, y_train)
        y_voting_pred = voting_ensemble.predict(X_val)
        y_voting_proba = voting_ensemble.predict_proba(X_val)[:, 1]
        voting_acc = accuracy_score(y_val, y_voting_pred)
        voting_auc = roc_auc_score(y_val, y_voting_proba)
        print(f"\n   Voting Ensemble - AUC: {voting_auc:.4f} | Accuracy: {voting_acc:.4f}")
    
    # 10. –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if best_model_name in ['LightGBM', 'XGBoost', 'CatBoost']:
        tuned_model = tune_best_model(best_model_name, X_train, y_train, X_val, y_val)
    
    # 11. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "="*100)
    print(" "*35 + "üìã –ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*100)
    
    print("\n1. –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò:")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ {results_df.iloc[0]['Model']} —Å AUC = {results_df.iloc[0]['Val_AUC']:.4f}")
    
    print("\n2. –î–õ–Ø –ë–ê–õ–ê–ù–°–ê –°–ö–û–†–û–°–¢–¨/–ö–ê–ß–ï–°–¢–í–û:")
    # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º
    results_df['Score_per_second'] = results_df['Val_AUC'] / results_df['Training_Time']
    best_efficiency = results_df.nlargest(1, 'Score_per_second').iloc[0]
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ {best_efficiency['Model']} (AUC = {best_efficiency['Val_AUC']:.4f}, "
          f"–í—Ä–µ–º—è = {best_efficiency['Training_Time']:.2f}—Å)")
    
    print("\n3. –î–õ–Ø –ü–†–û–î–ê–ö–®–ï–ù–ê:")
    print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Voting –∞–Ω—Å–∞–º–±–ª—å –∏–∑ —Ç–æ–ø-3 –º–æ–¥–µ–ª–µ–π –∏–ª–∏")
    print("   —Ç–æ–Ω–∫–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏")
    
    print("\n4. –î–õ–Ø –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í:")
    if LIGHTGBM_AVAILABLE:
        print("   LightGBM - –±—ã—Å—Ç—Ä—ã–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–π")
    else:
        print("   Random Forest - —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("-"*60)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df.drop('model_object', axis=1).to_csv('titanic_model_comparison_results.csv', index=False)
    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'titanic_model_comparison_results.csv'")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    try:
        import joblib
        joblib.dump(best_model, 'best_titanic_model.pkl')
        print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å ({best_model_name}) —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'best_titanic_model.pkl'")
    except:
        print("‚ö†Ô∏è –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ joblib: pip install joblib")
    
    print("\n" + "="*100)
    print(" "*40 + "‚ú® –ì–û–¢–û–í–û! ‚ú®")
    print("="*100)
    
    return results_df, best_model


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
    results, model = main()
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\nüí° –°–æ–≤–µ—Ç: –î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ —ç—Ç–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞")
    print("   –ù–∞–ø—Ä–∏–º–µ—Ä: comparison = ModelComparison() –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
